from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import numpy as np

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = AutoModel.from_pretrained("google-bert/bert-base-uncased")
texts = ["The apple is tasty", "The weather is bad", "I took my dog for a play"]  # Added another text for demonstration

def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]

def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    with torch.no_grad():
        model_output = model(**encoded_input)
    return cls_pooling(model_output)

if __name__ == '__main__':
    embeddings = get_embeddings(texts)
    print(embeddings.size())
    embeddings = embeddings.numpy()
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings.astype(np.float32))

    query_embedding = get_embeddings(["I took my cat for a walk"]).numpy()

    k = 1
    D, I = index.search(query_embedding[0:1], k)
    print("Distances:", D)
    print("Indices:", I)
    for i in I[0]:  
        print(f"Index {i} text: {texts[i]}")